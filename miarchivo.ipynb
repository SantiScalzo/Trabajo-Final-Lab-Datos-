{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247fdb22",
   "metadata": {},
   "source": [
    "Lo primero que hacemos es cargar todos las hojas a utilizar para posteriormente concatenarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec0f9011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos CSV encontrados: 6\n",
      "\n",
      "✅ Concatenación horizontal completada.\n",
      "Número de filas (filas originales): 16054\n",
      "Número de columnas totales (sumatoria de columnas de todos los CSVs): 137\n",
      "\n",
      "DataFrame Combinado (Primeras filas y todas las columnas):\n",
      "                   DIA      HORA  Agua Planta (Hl)  Agua Elaboracion (Hl)  \\\n",
      "0  2021-07-01 00:00:00  00:00:00               0.0                    0.0   \n",
      "1  2021-07-01 00:00:00  01:00:00               0.0                    0.0   \n",
      "2  2021-07-01 00:00:00  02:00:00             490.0                   28.0   \n",
      "3  2021-07-01 00:00:00  03:00:00             650.0                   52.0   \n",
      "4  2021-07-01 00:00:00  04:00:00             880.0                   75.0   \n",
      "\n",
      "   Agua Bodega (Hl)  Agua Cocina (Hl)  Agua Dilucion (Hl)  Agua Envasado (Hl)  \\\n",
      "0               0.0               0.0                 0.0                 0.0   \n",
      "1               0.0               0.0                 0.0                 0.0   \n",
      "2              28.0               0.0                 0.0                 0.0   \n",
      "3              52.0               0.0                 0.0                 1.0   \n",
      "4              75.0               0.0                 0.0                 1.0   \n",
      "\n",
      "   Agua Servicios (Hl)  Planta de agua (Hl)  ...      HORA  Hl de Mosto  \\\n",
      "0                  0.0                  0.0  ...  00:00:00          0.0   \n",
      "1                  0.0                  0.0  ...  01:00:00          0.0   \n",
      "2                462.0                487.2  ...  02:00:00          0.0   \n",
      "3                597.0                645.8  ...  03:00:00          0.0   \n",
      "4                804.0                873.5  ...  04:00:00          0.0   \n",
      "\n",
      "   Hl Cerveza Cocina  Hl Producido Bodega  Hl Cerveza Filtrada  \\\n",
      "0                0.0                  0.0                  0.0   \n",
      "1                0.0                  0.0                  0.0   \n",
      "2                0.0                  0.0                  0.0   \n",
      "3                0.0                  0.0                  0.0   \n",
      "4                0.0                  0.0                  0.0   \n",
      "\n",
      "   Hl Cerveza Envasada  Hl Cerveza L2  Hl Cerveza L3 Hl Cerveza L4  \\\n",
      "0                  0.0            0.0            0.0           0.0   \n",
      "1                  0.0            0.0            0.0           0.0   \n",
      "2                  0.0            0.0            0.0           0.0   \n",
      "3                  0.0            0.0            0.0           0.0   \n",
      "4                  0.0            0.0            0.0           0.0   \n",
      "\n",
      "  Hl Cerveza L5  \n",
      "0           0.0  \n",
      "1           0.0  \n",
      "2           0.0  \n",
      "3           0.0  \n",
      "4           0.0  \n",
      "\n",
      "[5 rows x 137 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Define la ruta de la carpeta (utilizando la ruta proporcionada)\n",
    "ruta_carpeta = Path(r'C:\\Users\\scalz\\Documents\\Documentos\\Lab_Datos\\TPfinal\\Archivos_xlsx')\n",
    "\n",
    "# 2. Encuentra la lista de archivos CSV dentro de esa carpeta\n",
    "archivos_csv = list(ruta_carpeta.glob('*.csv'))\n",
    "\n",
    "if not archivos_csv:\n",
    "    print(\"⚠️ ¡Atención! No se encontraron archivos CSV en la carpeta. Verifica que la extensión sea .csv.\")\n",
    "else:\n",
    "    # 3. Lee cada archivo CSV y crea una lista de DataFrames\n",
    "    print(f\"Archivos CSV encontrados: {len(archivos_csv)}\")\n",
    "    lista_dataframes = [pd.read_csv(archivo) for archivo in archivos_csv]\n",
    "\n",
    "    # 4. CONCATENACIÓN HORIZONTAL: Une todos los DataFrames por COLUMNAS\n",
    "    df_combinado_horizontal = pd.concat(\n",
    "        lista_dataframes, \n",
    "        axis=1,             # <--- ESTE ES EL CAMBIO CLAVE: Une por columnas.\n",
    "        join='outer'        # Por defecto, usa 'outer' (mantiene todas las filas), pero con filas iguales 'inner' u 'outer' dan el mismo resultado.\n",
    "    )\n",
    "\n",
    "    # 5. Verificación Final\n",
    "    print(\"\\n✅ Concatenación horizontal completada.\")\n",
    "    \n",
    "    # El número de filas debería ser el mismo que el de cualquier archivo original.\n",
    "    print(f\"Número de filas (filas originales): {df_combinado_horizontal.shape[0]}\") \n",
    "    \n",
    "    # El número de columnas es la suma de las columnas de todos los archivos originales.\n",
    "    print(f\"Número de columnas totales (sumatoria de columnas de todos los CSVs): {df_combinado_horizontal.shape[1]}\") \n",
    "    \n",
    "    print(\"\\nDataFrame Combinado (Primeras filas y todas las columnas):\")\n",
    "    print(df_combinado_horizontal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f2d2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_combinado_horizontal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7e69fa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos CSV encontrados: 6\n",
      "\n",
      "✅ Concatenación horizontal completada.\n",
      "Número de filas (filas originales): 15649\n",
      "Número de columnas totales (sumatoria de columnas de todos los CSVs): 137\n",
      "\n",
      "DataFrame Combinado (Primeras filas y todas las columnas):\n",
      "                   DIA      HORA  Agua Planta (Hl)  Agua Elaboracion (Hl)  \\\n",
      "0  2022-07-01 00:00:00  02:00:00             466.0                   43.0   \n",
      "1  2022-07-01 00:00:00  03:00:00             915.0                   86.0   \n",
      "2  2022-07-01 00:00:00  04:00:00            1364.0                  128.0   \n",
      "3  2022-07-01 00:00:00  05:00:01            1986.0                  170.0   \n",
      "4  2022-07-01 00:00:00  06:00:01            2432.0                  213.0   \n",
      "\n",
      "   Agua Bodega (Hl)  Agua Cocina (Hl)  Agua Dilucion (Hl)  Agua Envasado (Hl)  \\\n",
      "0              43.0               0.0                 0.0                26.0   \n",
      "1              86.0               0.0                 0.0                52.0   \n",
      "2             128.0               0.0                 0.0                78.0   \n",
      "3             170.0               0.0                 0.0               103.0   \n",
      "4             213.0               0.0                 0.0               129.0   \n",
      "\n",
      "   Agua Servicios (Hl)  Planta de agua (Hl)  ...      HORA  Hl de Mosto  \\\n",
      "0                397.0                487.7  ...  02:00:00       3309.0   \n",
      "1                777.0                958.4  ...  03:00:00       3309.0   \n",
      "2               1158.0               1429.2  ...  04:00:00       3309.0   \n",
      "3               1713.0               2072.0  ...  05:00:01       3309.0   \n",
      "4               2090.0               2539.7  ...  06:00:01       3309.0   \n",
      "\n",
      "   Hl Cerveza Cocina  Hl Producido Bodega  Hl Cerveza Filtrada  \\\n",
      "0            4334.79             6590.895               4435.0   \n",
      "1            4334.79             6590.895               4435.0   \n",
      "2            4334.79             6590.895               4435.0   \n",
      "3            4334.79             6590.895               4435.0   \n",
      "4            4334.79             6590.895               4435.0   \n",
      "\n",
      "   Hl Cerveza Envasada  Hl Cerveza L2  Hl Cerveza L3 Hl Cerveza L4  \\\n",
      "0               8847.0         2205.0         3164.0        3334.0   \n",
      "1               8847.0         2205.0         3164.0        3334.0   \n",
      "2               8847.0         2205.0         3164.0        3334.0   \n",
      "3               8847.0         2205.0         3164.0        3334.0   \n",
      "4               8847.0         2205.0         3164.0        3334.0   \n",
      "\n",
      "  Hl Cerveza L5  \n",
      "0         144.0  \n",
      "1         144.0  \n",
      "2         144.0  \n",
      "3         144.0  \n",
      "4         144.0  \n",
      "\n",
      "[5 rows x 137 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Define la ruta de la carpeta (utilizando la ruta proporcionada)\n",
    "ruta_carpeta = Path(r'C:\\Users\\scalz\\Documents\\Documentos\\Lab_Datos\\TPfinal\\Archivos_xlsx2')\n",
    "\n",
    "# 2. Encuentra la lista de archivos CSV dentro de esa carpeta\n",
    "archivos_csv = list(ruta_carpeta.glob('*.csv'))\n",
    "\n",
    "if not archivos_csv:\n",
    "    print(\"⚠️ ¡Atención! No se encontraron archivos CSV en la carpeta. Verifica que la extensión sea .csv.\")\n",
    "else:\n",
    "    # 3. Lee cada archivo CSV y crea una lista de DataFrames\n",
    "    print(f\"Archivos CSV encontrados: {len(archivos_csv)}\")\n",
    "    lista_dataframes = [pd.read_csv(archivo) for archivo in archivos_csv]\n",
    "\n",
    "    # 4. CONCATENACIÓN HORIZONTAL: Une todos los DataFrames por COLUMNAS\n",
    "    df_combinado_horizontal = pd.concat(\n",
    "        lista_dataframes, \n",
    "        axis=1,             # <--- ESTE ES EL CAMBIO CLAVE: Une por columnas.\n",
    "        join='outer'        # Por defecto, usa 'outer' (mantiene todas las filas), pero con filas iguales 'inner' u 'outer' dan el mismo resultado.\n",
    "    )\n",
    "\n",
    "    # 5. Verificación Final\n",
    "    print(\"\\n✅ Concatenación horizontal completada.\")\n",
    "    \n",
    "    # El número de filas debería ser el mismo que el de cualquier archivo original.\n",
    "    print(f\"Número de filas (filas originales): {df_combinado_horizontal.shape[0]}\") \n",
    "    \n",
    "    # El número de columnas es la suma de las columnas de todos los archivos originales.\n",
    "    print(f\"Número de columnas totales (sumatoria de columnas de todos los CSVs): {df_combinado_horizontal.shape[1]}\") \n",
    "    \n",
    "    print(\"\\nDataFrame Combinado (Primeras filas y todas las columnas):\")\n",
    "    print(df_combinado_horizontal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f87f5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df_combinado_horizontal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "649c86e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Columnas duplicadas en df1 ---\n",
      "['DIA', 'HORA', 'DIA', 'HORA', 'DIA', 'HORA', 'DIA', 'HORA', 'DIA', 'HORA']\n",
      "\n",
      "---------------------------------\n",
      "--- Columnas duplicadas en df2 ---\n",
      "['DIA', 'HORA', 'DIA', 'HORA', 'DIA', 'HORA', 'DIA', 'HORA', 'DIA', 'HORA']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Identificar en df1\n",
    "columnas_df1 = df1.columns\n",
    "duplicadas_df1 = columnas_df1[columnas_df1.duplicated()]\n",
    "\n",
    "print(\"--- Columnas duplicadas en df1 ---\")\n",
    "if not duplicadas_df1.empty:\n",
    "    print(duplicadas_df1.tolist())\n",
    "else:\n",
    "    print(\"No se encontraron columnas duplicadas en df1.\")\n",
    "\n",
    "print(\"\\n---------------------------------\")\n",
    "\n",
    "# 2. Identificar en df2\n",
    "columnas_df2 = df2.columns\n",
    "duplicadas_df2 = columnas_df2[columnas_df2.duplicated()]\n",
    "\n",
    "print(\"--- Columnas duplicadas en df2 ---\")\n",
    "if not duplicadas_df2.empty:\n",
    "    print(duplicadas_df2.tolist())\n",
    "else:\n",
    "    print(\"No se encontraron columnas duplicadas en df2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e5e46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df=[df1,df2]\n",
    "\n",
    "df1 = df1.loc[:, ~df1.columns.duplicated()]\n",
    "df2 = df2.loc[:, ~df2.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "28675d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Columnas duplicadas en df1 ---\n",
      "No se encontraron columnas duplicadas en df1.\n",
      "\n",
      "---------------------------------\n",
      "--- Columnas duplicadas en df2 ---\n",
      "No se encontraron columnas duplicadas en df2.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Identificar en df1\n",
    "columnas_df1 = df1.columns\n",
    "duplicadas_df1 = columnas_df1[columnas_df1.duplicated()]\n",
    "\n",
    "print(\"--- Columnas duplicadas en df1 ---\")\n",
    "if not duplicadas_df1.empty:\n",
    "    print(duplicadas_df1.tolist())\n",
    "else:\n",
    "    print(\"No se encontraron columnas duplicadas en df1.\")\n",
    "\n",
    "print(\"\\n---------------------------------\")\n",
    "\n",
    "# 2. Identificar en df2\n",
    "columnas_df2 = df2.columns\n",
    "duplicadas_df2 = columnas_df2[columnas_df2.duplicated()]\n",
    "\n",
    "print(\"--- Columnas duplicadas en df2 ---\")\n",
    "if not duplicadas_df2.empty:\n",
    "    print(duplicadas_df2.tolist())\n",
    "else:\n",
    "    print(\"No se encontraron columnas duplicadas en df2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4984d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df=[df1,df2]\n",
    "df_concatenado = pd.concat(vector_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6c06500e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo 'datos_para_descargar.csv' creado y listo para descargar en tu directorio actual.\n"
     ]
    }
   ],
   "source": [
    "# 1. Importar la librería Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Suponiendo que 'mi_dataframe' es el DataFrame que quieres guardar\n",
    "# mi_dataframe = pd.DataFrame(...)\n",
    "\n",
    "# 2. Generar y guardar el archivo CSV\n",
    "nombre_archivo = 'datos_para_descargar.csv'\n",
    "\n",
    "df_concatenado.to_csv(\n",
    "    nombre_archivo,\n",
    "    index=False,      # NO incluir los índices de fila del DataFrame\n",
    "    encoding='utf-8'  # Usar una codificación estándar que soporte caracteres especiales\n",
    ")\n",
    "\n",
    "print(f\"✅ Archivo '{nombre_archivo}' creado y listo para descargar en tu directorio actual.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19d2f1",
   "metadata": {},
   "source": [
    "A partir de esto comienzo con el EDA. Se viene lo chido agustin MarxSioni(sta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471755f2",
   "metadata": {},
   "source": [
    "Lo primero que hago es detrectar valores faltantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f0c6b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Aire Servicios (M3)        142\n",
       "Vapor L3                   194\n",
       "Restos Planta (Kw)         199\n",
       "Agua Elaboracion (Hl)      203\n",
       "Agua Servicios (Hl)        203\n",
       "                         ...  \n",
       "Agua Linea 5/Hl          11494\n",
       "Aire L5 / Hl             11494\n",
       "ET Linea 5/Hl            11494\n",
       "Vapor _Vapor_L5 (KG)     16128\n",
       "Vapor_L5 (KG)            16387\n",
       "Length: 128, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenado.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "02dd2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Lo que quiero hacer es guardar todas las columnas donde los nulos sean mayor a 5000(aprox un 5% del dataset) eliminarlas ya que creo que faltan muchos datos para imputarlas\n",
    "null_counts = df_concatenado.isnull().sum()\n",
    "# 2. Filtrar y obtener los nombres de las columnas que cumplen la condición\n",
    "columnas_con_muchos_nulos = null_counts[null_counts > 5000].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "01524584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vapor _Vapor_L5 (KG)',\n",
       " 'EE Linea 2 / Hl',\n",
       " 'EE Linea 3 / Hl',\n",
       " 'Agua Linea 2/Hl',\n",
       " 'Agua Linea 3/Hl',\n",
       " 'Agua Linea 5/Hl',\n",
       " 'ET Linea 2/Hl',\n",
       " 'ET Linea 3/Hl',\n",
       " 'ET Linea 5/Hl',\n",
       " 'Aire L2 / Hl',\n",
       " 'Aire L3 / Hl',\n",
       " 'Aire L5 / Hl',\n",
       " 'Vapor_L5 (KG)']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas_con_muchos_nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6bbaa",
   "metadata": {},
   "source": [
    "Faltan datos de TODAS las columnas desde el 16/3/22 hasta el 1/7/22. Que hacemos? las imputamos nomas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1071ae",
   "metadata": {},
   "source": [
    "A partir de lo visto yo borraria las 13 columnas donde los valores nulos representan mas del 5% de la cantidad de datos. \n",
    "Antes voy a ver que tan correlacionadas estan con la variable objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16787733",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf02b4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlación entre 'Frio (Kw)' y 'Vapor _Vapor_L5 (KG)':\n",
      "[[1.         0.00119096]\n",
      " [0.00119096 1.        ]]\n",
      "Correlación entre 'Frio (Kw)' y 'EE Linea 2 / Hl':\n",
      "[[ 1.         -0.00162782]\n",
      " [-0.00162782  1.        ]]\n",
      "Correlación entre 'Frio (Kw)' y 'EE Linea 3 / Hl':\n",
      "[[ 1.00000000e+00 -1.49440072e-04]\n",
      " [-1.49440072e-04  1.00000000e+00]]\n",
      "Correlación entre 'Frio (Kw)' y 'Agua Linea 2/Hl':\n",
      "[[ 1.         -0.00139238]\n",
      " [-0.00139238  1.        ]]\n",
      "Correlación entre 'Frio (Kw)' y 'Agua Linea 3/Hl':\n",
      "[[ 1.00000000e+00 -2.07863676e-04]\n",
      " [-2.07863676e-04  1.00000000e+00]]\n",
      "Correlación entre 'Frio (Kw)' y 'Agua Linea 5/Hl':\n",
      "[[ 1.        -0.0010289]\n",
      " [-0.0010289  1.       ]]\n",
      "Correlación entre 'Frio (Kw)' y 'ET Linea 2/Hl':\n",
      "[[ 1.         -0.00130219]\n",
      " [-0.00130219  1.        ]]\n",
      "Correlación entre 'Frio (Kw)' y 'ET Linea 3/Hl':\n",
      "[[ 1.00000000e+00 -1.62934298e-04]\n",
      " [-1.62934298e-04  1.00000000e+00]]\n",
      "Correlación entre 'Frio (Kw)' y 'ET Linea 5/Hl':\n",
      "[[ 1.         -0.00104183]\n",
      " [-0.00104183  1.        ]]\n",
      "Correlación entre 'Frio (Kw)' y 'Aire L2 / Hl':\n",
      "[[ 1.         -0.00138926]\n",
      " [-0.00138926  1.        ]]\n",
      "Correlación entre 'Frio (Kw)' y 'Aire L3 / Hl':\n",
      "[[ 1.00000000e+00 -1.59033116e-04]\n",
      " [-1.59033116e-04  1.00000000e+00]]\n",
      "Correlación entre 'Frio (Kw)' y 'Aire L5 / Hl':\n",
      "[[ 1.00000000e+00 -9.18216573e-04]\n",
      " [-9.18216573e-04  1.00000000e+00]]\n",
      "Correlación entre 'Frio (Kw)' y 'Vapor_L5 (KG)':\n",
      "[[1.         0.25125967]\n",
      " [0.25125967 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Aca lo que hago es eliminar los null en cualquiera de las dos columnas a modo de pruea para solamente ver\n",
    "#que tan correlacionada estan las columnas que quiero borrar (muchos null) y la objetivo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Tu bucle original con la modificación\n",
    "for i in columnas_con_muchos_nulos:\n",
    "    # Crear un DataFrame temporal con solo las dos columnas\n",
    "    temp_df = df_concatenado[['Frio (Kw)', i]].copy()\n",
    "\n",
    "    # **Eliminar las filas donde haya algún NaN en estas dos columnas**\n",
    "    temp_df_cleaned = temp_df.dropna()\n",
    "    \n",
    "    # Comprobar si aún quedan datos para correlacionar\n",
    "    if len(temp_df_cleaned) > 1: \n",
    "        # Calcular la correlación en los datos limpios\n",
    "        correlacion = np.corrcoef(temp_df_cleaned['Frio (Kw)'], temp_df_cleaned[i])\n",
    "        print(f\"Correlación entre 'Frio (Kw)' y '{i}':\")\n",
    "        print(correlacion)\n",
    "    else:\n",
    "        print(f\"No hay suficientes datos válidos (después de eliminar NaN) para calcular la correlación entre 'Frio (Kw)' y '{i}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b15782",
   "metadata": {},
   "source": [
    "Quiero ver por las dudas si no me esta tomando los valores de 0 como valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7d768649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vals=np.where(df_concatenado['EE Linea 2 / Hl']==0)\n",
    "\n",
    "indices_de_cero = vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5a1df4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4477"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices_de_cero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "035cf6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5612\n"
     ]
    }
   ],
   "source": [
    "print(df_concatenado['EE Linea 2 / Hl'].isnull().sum())\n",
    "#Como son valores diferentes veo que los valores nulos son diferentes de 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72667e2a",
   "metadata": {},
   "source": [
    "Lo que voy a hacer ahora es borrar esas 13 columnas (luego se podria aumentar el \"umbral\" y por ende borrar mas columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2de36eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in columnas_con_muchos_nulos: \n",
    "    df_concatenado=df_concatenado.drop(i,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d5c76",
   "metadata": {},
   "source": [
    "Ahora separo el conjunto de datos en train y test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2d62f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "y=df_concatenado['Frio (Kw)']\n",
    "x_train, x_test, y_train, y_test=skl.model_selection.train_test_split(df_concatenado, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda86ad",
   "metadata": {},
   "source": [
    "Ahora imputo los valores nulos con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5e481681",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2022-12-13 00:00:00'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_632\\2032681648.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m imputer=skl.impute.KNNImputer(n_neighbors=\u001b[32m10\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m x_train=imputer.fit_transform(x_train)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#y_train=imputer.fit_transform(y_train)\u001b[39;00m\n\u001b[32m      4\u001b[39m x_test=imputer.fit_transform(x_test)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#y_test=imputer.fit_transform(y_test)\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\scalz\\.conda\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m     @wraps(f)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    318\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m             return_tuple = (\n",
      "\u001b[32mc:\\Users\\scalz\\.conda\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    890\u001b[39m                 )\n\u001b[32m    891\u001b[39m \n\u001b[32m    892\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m             \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, **fit_params).transform(X)\n\u001b[32m    895\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m             \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[32mc:\\Users\\scalz\\.conda\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1361\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m                 )\n\u001b[32m   1364\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32mc:\\Users\\scalz\\.conda\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\impute\\_knn.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    233\u001b[39m             ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    234\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m             ensure_all_finite = \u001b[33m\"allow-nan\"\u001b[39m\n\u001b[32m    236\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m         X = validate_data(\n\u001b[32m    238\u001b[39m             self,\n\u001b[32m    239\u001b[39m             X,\n\u001b[32m    240\u001b[39m             accept_sparse=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[32mc:\\Users\\scalz\\.conda\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2950\u001b[39m             out = y\n\u001b[32m   2951\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2952\u001b[39m             out = X, y\n\u001b[32m   2953\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2955\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2957\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32mc:\\Users\\scalz\\.conda\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1050\u001b[39m                         )\n\u001b[32m   1051\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1052\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1053\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1055\u001b[39m                 raise ValueError(\n\u001b[32m   1056\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1057\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32mc:\\Users\\scalz\\.conda\\envs\\cervecera_env\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    754\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    755\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    756\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    758\u001b[39m \n\u001b[32m    759\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    760\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\scalz\\.conda\\envs\\cervecera_env\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2167\u001b[39m             )\n\u001b[32m   2168\u001b[39m         values = self._values\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2172\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2173\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2174\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '2022-12-13 00:00:00'"
     ]
    }
   ],
   "source": [
    "imputer=skl.impute.KNNImputer(n_neighbors=10)\n",
    "x_train=imputer.fit_transform(x_train)\n",
    "#y_train=imputer.fit_transform(y_train)\n",
    "x_test=imputer.fit_transform(x_test)\n",
    "#y_test=imputer.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d47f47",
   "metadata": {},
   "source": [
    "Ahora tambien debo detectar los valores atipicos de cada una de las columnas para posteriormente imputarlos, seguramente con knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883a5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scalz\\AppData\\Local\\Temp\\ipykernel_632\\1903916467.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  rango_fechas_completo = pd.date_range(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo 'fechas_faltantes_completo.xlsx' generado con éxito.\n",
      "El DataFrame contiene 2569 muestras.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Definir las fechas de inicio y fin como cadenas\n",
    "fecha_inicio = '2022-03-16 00:00:00'\n",
    "fecha_fin = '2022-07-01 00:00:00'\n",
    "\n",
    "# 2. Generar el rango de fechas con una frecuencia de una hora ('H')\n",
    "# El resultado es un objeto DatetimeIndex que incluye la fecha de inicio y la fecha de fin.\n",
    "rango_fechas_completo = pd.date_range(\n",
    "    start=fecha_inicio, \n",
    "    end=fecha_fin, \n",
    "    freq='H'\n",
    ")\n",
    "\n",
    "# 3. Convertir el rango a un DataFrame de pandas\n",
    "df_fechas = pd.DataFrame({\n",
    "    'Fecha_Hora': rango_fechas_completo,\n",
    "    'ID_Muestra': range(1, len(rango_fechas_completo) + 1)\n",
    "})\n",
    "\n",
    "# 4. Exportar el DataFrame a un archivo Excel\n",
    "nombre_archivo = \"fechas_faltantes_completo.xlsx\"\n",
    "df_fechas.to_excel(\n",
    "    nombre_archivo, \n",
    "    index=False, # No incluir el índice de pandas en el archivo\n",
    "    sheet_name='Rango_Horario'\n",
    ")\n",
    "\n",
    "print(f\"✅ Archivo '{nombre_archivo}' generado con éxito.\")\n",
    "print(f\"El DataFrame contiene {len(df_fechas)} muestras.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c652a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cervecera_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
